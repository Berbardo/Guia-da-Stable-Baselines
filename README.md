# üëæ Guia da Stable Baselines 

 Guia de como utilizar a biblioteca [Stable Baselines](https://github.com/hill-a/stable-baselines) para projetos de Aprendizagem por refor√ßo.

O guia completo encontra-se neste notebook:

### [Guia Completo em Notebook](Guia%20da%20Stable%20Baselines.ipynb)

Entretanto, caso voc√™ n√£o queira ver os exemplos em c√≥digo, o texto do guia est√° presente a seguir.

## √çndice

- [O que √© Stable Baselines?](#o-que-√©-stable-baselines?)
- [Instala√ß√£o](#instala√ß√£o)
- [Como usar Stable Baselines?](#Como-usar-Stable-Baselines?)
  - [Gym](#Gym)
  - [O que √© um Ambiente?](#O-que-√©-um-Ambiente?)
  - [Como Funciona um Ambiente do Gym?](#Como-Funciona-um-Ambiente-do-Gym?)
  - [Criando um Ambiente](#Criando-um-Ambiente)
  - [Criando um Agente](#Criando-um-Agente)

## O que √© Stable Baselines?

A **Stable Baselines** √© uma biblioteca de Aprendizagem por Refor√ßo que implementa diversos algoritmos de agentes de RL, al√©m de v√°rias funcionalidades √∫teis para o treinamento de um agente. Suas implementa√ß√µes s√£o bem simples e intuitivas, mas sem deixarem de ser otimizadas e poderosas, buscando facilitar o desenvolvimento de projetos de refor√ßo de alta qualidade.

![Logo](https://github.com/hill-a/stable-baselines/raw/master/docs//_static/img/logo.png "Logo da Stable Baselines")

## Instala√ß√£o

## Como usar Stable Baselines?

### Gym

O **[Gym](https://gym.openai.com/)** √© uma biblioteca desenvolvida pela OpenAI que cont√©m v√°rias implementa√ß√µes prontas de ambientes de Aprendizagem por Refor√ßo. Ela √© muito utilizada quando se quer testar um algoritmo de agente sem ter o trabalho de programar seu pr√≥prio ambiente.

<img src="https://camo.githubusercontent.com/25043fb622d3f9115a263fb71c61adb08c1d7790/68747470733a2f2f7072617665656e702e636f6d2f70726f6a656374732f484f4941574f472f6f75747075742e676966" alt="Exemplos de Ambientes do Gym" class="inline"/>

### O que √© um Ambiente?

Um **Ambiente** de Aprendizagem por Refor√ßo √© um espa√ßo que representa o nosso problema, √© o objeto com o qual o nosso agente deve interagir para cumprir sua fun√ß√£o. Isso significa que o agente toma **a√ß√µes** nesse ambiente, e recebe **recompensas** dele com base na qualidade de sua tomada de decis√µes.

Todos os ambientes s√£o dotados de um **espa√ßo de observa√ß√µes**, que √© a forma pela qual o agente recebe informa√ß√µes e deve se basear para a tomada de decis√µes, e um **espa√ßo de a√ß√µes**, que especifica as a√ß√µes poss√≠veis do agente. No xadrez, por exemplo, o espa√ßo de observa√ß√µes seria o conjunto de todas as configura√ß√µes diferentes do tabuleiro, e o espa√ßo de a√ß√µes seria o conjunto de todos os movimentos permitidos.

<img src="https://www.raspberrypi.org/wp-content/uploads/2016/08/giphy-1-1.gif" alt="Uma A√ß√£o do Xadrez" class="inline"/>

### Como Funciona um Ambiente do Gym?

Agora que voc√™ j√° sabe o que √© um ambiente, √© preciso entender como nosso agente interage efetivamente com ele. Todos os ambientes do Gym possuem alguns m√©todos simples para facilitar a comunica√ß√£o com eles:

<br>

| M√©todo               | Funcionalidade                                          |
| :------------------- |:------------------------------------------------------- |
| reset()              | Inicializa o ambiente e recebe a observa√ß√£o inicial     |
| step(action)         | Executa uma a√ß√£o e recebe a observa√ß√£o e a recompensa   |
| render()             | Renderiza o ambiente                                    |
| close()              | Fecha o ambiente                                        |

<br>

Assim, o c√≥digo para interagir com o ambiente costuma seguir o seguinte modelo:

---

```python
ambiente = gym.make("Nome do Ambiente")                         # Cria o ambiente
observa√ß√£o = ambiente.reset()                                   # Inicializa o ambiente
acabou = False

while not acabou:
    ambiente.render()                                           # Renderiza o ambiente
    observa√ß√£o, recompensa, acabou, info = ambiente.step(a√ß√£o)  # Executa uma a√ß√£o
    
ambiente.close()                                                # Fecha o ambiente
```

---

### Criando um Ambiente

Para utilizar um dos ambientes do Gym, n√≥s utilizamos a fun√ß√£o ```gym.make()```, passando o nome do ambiente desejado como par√¢metro e guardando seu valor retornado em uma vari√°vel que chamaramos de ```env```. A lista com todos os ambiente pode ser encontrada [aqui](https://gym.openai.com/envs/#classic_control).

#### Exemplo - CartPole

Para exemplificar, vamos pensar no ambiente ```CartPole-v1```, um ambiente bem simples que modela um p√™ndulo invertido em cima de um carrinho buscando seu estado de equil√≠brio.

<img src="https://miro.medium.com/max/1200/1*jLj9SYWI7e6RElIsI3DFjg.gif" width="400px" alt="Ambiente do CartPole-v1" class="inline"/>

Antes de treinar qualquer agente, primeiro √© preciso entender melhor quais as caracter√≠sticas do nosso ambiente.

O **Espa√ßo de Observa√ß√£o** do CartPole √© definido por 4 informa√ß√µes:

<br>

|     | Informa√ß√£o                         | Min     | Max    |
| :-- | :--------------------------------- | :-----: | :----: |
| 0   | Posi√ß√£o do Carrinho                | -4.8    | 4.8    |
| 1   | Velocidade do Carrinho             | -Inf    | Inf    |
| 2   | √Çngulo da Barra                    | -24 deg | 24 deg |
| 3   | Velocidade na Extremidade da Barra | -Inf    | Inf    |

<br>

Dessa forma, a cada instante recebemos uma lista da observa√ß√£o com o seguinte formato:

```[-3.3715708e+00 -2.6997593e+38  2.7833584e-01  2.0276438e+38]```

J√° o **Espa√ßo de A√ß√£o** √© composto por duas a√ß√µes √∫nicas: mover o carrinho para a **esquerda** ou para a **direita**.

Quando queremos mover o carrinho para a esquerda, fazemos um `env.step(0)`; quando queremos mov√™-lo para a direita, enviamos um `env.step(1)`